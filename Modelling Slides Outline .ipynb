{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considered 3 main models: \n",
    "    1. Logistic Regression\n",
    "    2. Random Forest\n",
    "    3. Boosted Trees\n",
    "- Split data into train and test sets with an 80-20 split in chronological order to avoid leakage as per the Kaggle competition rules (i.e. didn't want to use information about future shots in the training phase).\n",
    "- Tuned each model using k-fold cross validation on the training set (k=10), with log-loss as the metric. This gave an estimate of each model's ability to generalise as well as its variance.\n",
    "- Selected the best model using this approach, and finally assessed its performance using the held-out test set at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initially considered a model with just `shot_distance` as a predictor variable. This resulted in a cross validation accuracy of 0.5984 ± 0.0143 and a log-loss of 0.6674 ± 0.0072. This also allowed us to estimate the shot distance at which shot success probability falls to 50%, which occured at a distance of 8.3ft.\n",
    "- Then created a model using all of our predictor variables, but with L1 penalisation, where the penalisation coefficient and type of penalisation was found using a grid search. This should result in reduced variance and better generalisation to unseen data due to the simpler model. \n",
    "- This resulted in a cross validated accuracy of 0.6795 ± 0.0142 and log-loss of 0.6271 ± 0.0087."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carried out a grid search for the random forest hyperparameters, which resulted in `max_depth` of 17 and `min_samples_leaf` of 6. The cross validated accuracy was found to be 0.6796 ± 0.0146 and the log-loss 0.6116 ± 0.0105, so a significant improvement over logistic regression.\n",
    "- We also visualised the effect of regularization through varying `max_depth`, which showed how the cross-validated error rate decreased for increasingly deep trees until a cut-off where the error rate began to increase again, showing how regularization is required to avoid over-fitting.\n",
    "- The random forest was also able to give us an idea of which features were important to the model, using feature importance ranking. It was found that the `action_type` variable, which encoded which type of shot was taken, the `shot_distance` variable, and the `defensive_ranking` variable were all important in the Random Forest approach, which makes intuitive sense.\n",
    "- We also fit a simple decision tree (but didn't attempt to optimize the hyperparameters) in order to show what a typical tree in our random forest model might look like, and visualized this in tree form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient boosted trees achieved a cross validated accuracy of ... and log loss of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the above models on their cross-validated performance, the gradient boosted trees and random forest methods performed the best, which would be expected as they are ensembling methods. They also had reasonably low variance. The gradient boosted trees marginally outperfomed the random forest approach, and so we chose this as our final model.\n",
    "- Using our final model on the held-out test data, we obtained a accuracy of 0.6770 and log-loss of 0.6064. This would have placed us xx/in the top xx% in the Kaggle competition, however it should be noted that we did not use the same test data as the competition so we can't really directly compare this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
